---
title: "Excercise 2: GLM"
author: "Ã€lex Batlle, Luis Sierra, Aleix Torres"
date: "22/4/2020"
output: html_document
---

```{r setup, include=FALSE}
library(magrittr)
library(caret)
library(factoextra)
library(MASS)
```

#### Introduction

For this analysis, we will be considering a dataset titled "Musk" created by the AI Group at Arris Pharmaceutical Corporation, consisting of chemical molecules and variables concerning different physical measurements of several low-energy states of those molecules. The aim is to predict whether the aforementioned variables are musk or non-musk, which is a property about the final smell of the molecules, from the 6,598 instances provided. The class of the molecule is given by the final variable, *V169*, and so that is what we will be trying to predict.

To do so, we will apply a Generalised Linear Model using a logit transformation over a set of variables because we considered this to be the most adequate way to model our data given the characteristics of the problem at hand. When trying to explore how other GLMs would fit the data the results were very poor, and the features of the dataset all point towards using this kind of model.

```{r}
data <- read.table("clean2.data", sep=',')
```

This is what a couple of molecules look like, since there are many configurations for the same molecule. We can see that if one such molecule is marked as a musk, then all configurations are marked as so.

```{r}
Musk.211 <- data[data$V1 == "MUSK-211",]
Musk.212 <- data[data$V1 == "MUSK-212",]
Musk.213 <- data[data$V1 == "MUSK-213",]
```

The values for the variables from *V3* to *V168* are very irregularly distributed, as can be seen when looking at the boxplots or histograms of the data. Moreover, we can check for missing datapoints, but when doing so we found none, and the authors of the dataset assured that there were no missing attribute fields, so no further treatment of the data was necessary.

#### Preliminary Considerations

Firstly, it is important to observe that given the 168 attributes (excluding the class) and the relatively high number of datapoints, using a GLM over all of the variables was unfeasible due to the lack of computational power. Despite generating the model without too many issues, when using the *step()* command to eliminate redundant variables the time it took was quite long, and the computer often crashed, and so the command in the chunk below is written as a comment.

```{r}
variables <- paste("V", 3:168, sep="")
fmla <- as.formula(paste("V169 ~ ", paste(variables, collapse= "+")))
giant_model <- glm(fmla, family = binomial, data = data)
# summary(giant_model)
# step(giant_model) -- Do not run
```

Therefore, we can consider models using only a subset of the variables in question, like for instance the first $n$ variables, as in the following chunk. Below is a GLM using a small number of variables.

```{r}
# Choose a small (<20) number of variables
n <- 20

# Formula for succint writing, excluding the factor variables due being the names
variables <- paste("V", 3:(n + 2), sep="")
fmla <- as.formula(paste("V169 ~ ", paste(variables, collapse= "+")))

# Simple GLM explaining V169 with respect to the first n variables
mod.simple <- glm(fmla, family = binomial, data = data)
summary(mod.simple)
```

That being said, the previous choice of variables is rather arbitrary, and so we could check all possible combination of $n$ variables and compare the models that result from such choices, with a total of $_{167} C_{n}$ models, which is a relatively large amount. Alternatively, we can consider techniques of dimensionality reduction in order not to have to resort to arbitrary choices of variables for the model. For instance, we could randomly sample the $n$ vaiables, or we could use principal component analysis, as in the following chunk.

```{r}
# PCA decomposition of the data
pca_data <- princomp(data[, -1:-2])
```

We can see from the results of the PCA that the first 35 principal components account for just over 95% of the total variability in the data. As such, we can produce a GLM using the principal components as variables to predict the value for the final *V169* variable.

It is important to take into account the fact that PCA by design is a dimensionality reduction method based around extracting the maximum variance and correlations between the datapoints at each step. As such, it is possible that the principal components are not the best at predicting the value for a single variable, such as *V169*, especially if there are many other variables which are highly correlated between them but which are not strongly related to out target variable. Below is a preliminary GLM from the results of a GLM using *dim=*35 dimensions.

```{r}
indices <- get_pca_ind(pca_data)

# Coordinates to use as variables
dim <- 35
coord <- indices$coord
reduced_data <- cbind(data[169], coord[,1:dim])
dimensions <- paste("Dim.", 1:dim, sep="")
pca_fmla <- as.formula(paste("V169 ~ ", paste(dimensions, collapse= "+")))

# GLM model from principal components using dim principal components
glm_pca <- glm(pca_fmla, family = binomial, data = reduced_data)

# Eliminating redundancy using Akaike Information Criterion (AIC)
pca_out <- step(glm_pca)
```

We can also create models reducing the dimensionality of the data by performing a Linear Discriminant Analysis (LDA) to project the data onto a lower-dimensional space, as shown in the following chunk.

```{r}
lda_model <- lda(V169 ~ ., data = data[,-1:-2]) 
plot(lda_model)
```

#### Model Evaluation:

The previously exposed models were all trained using the entirety of the data, and so they are prone to overfitting problems, given that there may be patterns in the data which are due to random chance rather than there being meaningful relationships between the variables. To sidestep such problems, we will use a method of evaluation of the models using K-fold cross validation.

This method is superior to Leave-One-Out Cross Validation (LOOCV) in that it is computationally less expensive, and given the relatively large amount of datapoints being handled, it preferable to prioritise computational considerations over precision ones. Moreover, James et al. (2014)[1] showed that K-fold CV often leads to more accurate values of the test error than LOOCV.

Below, we separate the data into training and learning to evaluate the performance of the models, in a 2:1 training/testing ratio, and after that perform the K-fold CV over the learning data.

```{r}
# Randomly separate into learning data and test data
set.seed(123)
sample_size <- floor(2/3 * nrow(data))
learning_indices <- sample(seq_len(nrow(data)), size = sample_size)
learning.data  <- data[learning_indices, ]
test.data <- data[-learning_indices, ]

# Using learning data for cross validation
K <- 5

# Transforming V169 to factor variable to perform cv
glm_learning.data <- learning.data
glm_learning.data$V169 <- factor(glm_learning.data$V169)

glm_model <- train(form = V169 ~ .,
               data = glm_learning.data[,-1:-2],
               trControl = trainControl(method = "cv", number = K),
               method = "glm",
               family = "binomial")

# Table of results along with accuracy
table(glm_learning.data$V169, predict(glm_model))
glm_model

# Real accuracy as calculated over test data
glm_predicted <- glm_model %>% predict(test.data, type = "raw")
glm_table <- table(test.data$V169, glm_predicted)
glm_accuracy <- (glm_table[1,1] + glm_table[2,2])/sum(glm_table)
glm_table
glm_accuracy
```

We can do the same to the data decomposed using PCA, as shown below.

```{r}
# Choice of principal components to be used
dim <- 20

# PCA on data
pca_learning.data <- princomp(learning.data[-1:-2])
principal_coord <- get_pca_ind(pca_learning.data)$coord

# Adjoining target variable to matrix of principal components
reduced_learning_data <- cbind(learning.data[169], principal_coord[,1:dim])

# Using learning data for cross validation
K <- 5

# Transforming V169 to factor variable to perform cv
reduced_learning_data$V169 <- factor(reduced_learning_data$V169)

pca_glm_model <- train(form = V169 ~ .,
               data = reduced_learning_data,
               trControl = trainControl(method = "cv", number = K),
               method = "glm",
               family = "binomial")

# Changing names of columns of learning data to those of the PCA
pca_test.data <- matrix((as.double(as.matrix(test.data[,-1:-2]))), nrow = 2200, ncol = 167) %*% loadings(pca_learning.data)[]
colnames(pca_test.data)[1:n] <- colnames(reduced_learning_data)[-1]

# Real accuracy as calculated over test data
pca_glm_predicted <- pca_glm_model %>% predict(pca_test.data, type = "raw")
pca_glm_table <- table(test.data$V169, pca_glm_predicted)
pca_glm_accuracy <- (pca_glm_table[1,1] + pca_glm_table[2,2])/sum(pca_glm_table)
pca_glm_table
pca_glm_accuracy
```

It is interesting to see that it is the case that the final GLM model is better than the final model after the dimensionality reduction, which makes sense given that there is a loss of information when considering only a subset of all principal components.

#### References

[1] James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated.
