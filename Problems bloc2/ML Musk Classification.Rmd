---
title: "Musk"
author: "Ã€lex Batlle, Luis Sierra, Aleix Torres"
date: "4/10/2020"
output: html_document
---

```{r}
data <- read.table("clean2.data", sep=',')
head(data)
```

Do not run this chunk, it takes too long simce the model has too many variables.

```{r}
giant_model <- glm(V169 ~ ., family = binomial, data = data)
summary(giant_model)
```

This what a couple of molecules look like, since there are many configurations for the same molecule.

```{r}
Musk.211 <- data[data$V1 == "MUSK-211",]
Musk.212 <- data[data$V1 == "MUSK-212",]
Musk.213 <- data[data$V1 == "MUSK-213",]
```

GLM using a small number of the variables.

```{r}
mod.simple <- glm(V169 ~ V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20, family = binomial, data = data)
summary(mod.simple)
step(mod.simple)
```

Dimensionality reduction using PCA.

```{r}
out.PCA <- princomp(data[3:169])
summary(out.PCA)
```

GLM from results of PCA

```{r}
library(factoextra)
pca_data <- princomp(data[-1:-2])
ind <- get_pca_ind(pca_data)
coord <- ind$coord
reduced_data <- cbind(data[169], coord[,1:10])
glm_pca <- glm(V169 ~ Dim.1 + Dim.2+ Dim.3 + Dim.4 + Dim.5 + Dim.6 + Dim.7 + Dim.8 + Dim.9 + Dim.10, family = binomial, data = reduced_data)
summary(glm_pca)
step(glm_pca)
```


LDA

```{r}
library(MASS)
#plot(data[, c(4, 7)], col = data[,169] + 2) #Ridiculous
data.lda <- lda(V167 ~ V3 + V4 + V5 + V6 + V7, data = data)
plot(data.lda)
```

#### Model Evaluation:

We could implement LOOCV. Below we separate the data into training and testing to evaluate the performance of the models, in an 8:2 training/testing ratio.

```{r}
library(magrittr)
library(caret)
set.seed(123)

# Function to separate into training data and test data
sample_size <- floor(2/3 * nrow(data))
train_indices <- sample(seq_len(nrow(data)), size = sample_size)
train.data  <- data[train_indices, ]
test.data <- data[-train_indices, ]

# Now define the models using only the training data and evaluate performance through testing

# GLM model consisting of logistic regression from the first 10 variables
simple10_glm_model <- glm(V169 ~ V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12, family = binomial, data = train.data)
prob_predictions <- simple10_glm_model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(prob_predictions > 0.5, 1, 0)
mean(predicted.classes == test.data$V169) # Accuracy of prediction

# GLM model consisting of a logistic regression from the first 20 variables
simple20_glm_model <- glm(V169 ~ V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22, family = binomial, data = train.data)
prob_predictions <- simple20_glm_model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(prob_predictions > 0.5, 1, 0)
mean(predicted.classes == test.data$V169) # Accuracy of prediction
```

```{r}
n <- 20
# GLM model consisting of a logistic regression from the PCA reduction of the data
pca_training_data <- princomp(train.data[-1:-2])
pca_ind <- get_pca_ind(pca_training_data)
principal_coord <- pca_ind$coord
reduced_training_data <- cbind(train.data[169], principal_coord[,1:n])
pca10_glm_model <- glm(V169 ~ Dim.1 + Dim.2 + Dim.3 + Dim.4 + Dim.5 + Dim.6 + Dim.7 + Dim.8 + Dim.9 + Dim.10 + Dim.11 + Dim.12 + Dim.13 + Dim.14 + Dim.15 + Dim.16 + Dim.17 + Dim.18 + Dim.19 + Dim.20, family = binomial, data = reduced_training_data)
# Changing names of columns of testing data
pca_test.data <- matrix((as.double(as.matrix(test.data[,-1:-2]))), nrow = 2200, ncol = 167) %*% (loadings(pca_training_data)[])
colnames(pca_test.data)[1:n] <- c(colnames(reduced_training_data)[-1])
# Validating results
prob_predictions_pca <- pca10_glm_model %>% predict(as.data.frame(pca_test.data), type = "response")
predicted_pca.classes <- ifelse(prob_predictions_pca > 0.5, 1, 0)
mean(predicted_pca.classes == test.data$V169)
```




