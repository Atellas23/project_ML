---
title: "wine project"
author: "Ã€lex, Luis, Aleix"
date: "25/4/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(class)
library(dplyr)
library(klaR)
library(magrittr)
library(caret)
library(randomForest)
library(cclust)
library(neural)
```



```{r}
our_seed <- 46290
red_w <- read.table("winequality-red.csv", sep=';', header = TRUE)
white_w <- read.table("winequality-white.csv", sep=';', header = TRUE)
```


```{r}
head(red_w)
dim(red_w)
head(white_w)
dim(white_w)
```

```{r}
red_w <- distinct(red_w)
white_w <- distinct(white_w)

wine <- rbind(red_w, white_w)

red_w <- red_w[c(-723,-964),]
white_w <- white_w[c(-2175, -2765),]
wine <- wine[c(-723,-964),]
```

```{r}
wine <- rbind(red_w, white_w)
wine$type <- c(rep('r', nrow(red_w)), rep('w', nrow(white_w)))
wine.quality <- wine$quality
wine.data <- subset(wine, select = 1:11)
```

# ----------------------------------------------------------------------------------------

Separate data into test and training
```{r}
# Randomly separate into learning data and test data
set.seed(our_seed)
sample_size <- floor(2/3 * nrow(wine))
training_indices <- sample(seq_len(nrow(wine)), size = sample_size)
training.data  <- wine[training_indices, ]
testing.data <- wine[-training_indices, ]
```
# ----------------------------------------------------------------------------------------

## TYPE CLASSIFICATION
Prepare the **cross-entropy** function to calculate the binary classification error:
```{r}
cross.entropy <- function(prediction, truth) {
  l <- levels(truth)
  if (length(levels(truth)) != 2) {
    return -1
  }
  class1 <- l[1]
  class2 <- l[2]
  truth[truth == class1] <- 1
  truth[truth == class2] <- 0
  prediction[prediction == class1] <- 1
  prediction[prediction == class2] <- 0
  
}
```


## RDA -> LDA
```{r}
set.seed(our_seed)
rda.model.loocv <- rda(x=training.data[,1:11], grouping=training.data$type, fold=nrow(training.data))
rda.model.loocv$regularization
```
We can see how lambda is 1 and gamma is very close to zero, so we should just use LDA.

```{r}
lda.kfcv <- function(target, dd, nfolds = 10, seed = 46290) {
  set.seed(seed)
  rownames(dd) <- NULL
  n <- nrow(dd)
  folds <- createFolds(seq_len(n), k = nfolds, list = TRUE)
  cv.err <- NULL
  t <- NULL
  p <- NULL
  for (fold in folds) {
    adj.data <- dd[-fold,]
    val <- dd[fold,]
    fmla <- as.formula(paste(target,"~.-quality"))
    lda.pred <- lda(fmla, data = adj.data) %>% predict(newdata = val)
    predicted.class <- lda.pred$class
    t <- c(t, val[,target])
    p <- c(p, predicted.class)
  }
  tab <- table(Truth = t, Pred = p)
}

(tab <- lda.kfcv(target = "type", training.data, nfolds = 10))
(LDA.kfcv.err <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab))

#lda.model <- lda(type ~ .-quality, data = training.data)
#loadings.lda <- as.matrix(training.data[,1:11]) %*% as.matrix(lda.model$scaling)
#plot(loadings.lda, col=c(training.data$type), ylab = "Projected coordinates", main = "LDA projection")
```

```{r}
#set.seed(our_seed)
#lda.model.loocv <- lda(x=wine.data, grouping=wine$type, CV=TRUE)
#(ct <- table(Truth=wine$type, Pred=lda.model.loocv$class))
#1 - sum(ct[row(ct)==col(ct)])/sum(ct)
```

```{r}
#set.seed(our_seed)
#qda.model.loocv <- qda(x=wine.data, grouping=wine$type, CV=TRUE)
#(ct <- table(Truth=wine$type, Pred=qda.model.loocv$class))
#1 - sum(ct[row(ct)==col(ct)])/sum(ct)
```


# KNN

```{r}
loop.k <- function(mydata, mytargets, upper.bound)
{
  errors <- matrix(nrow=upper.bound, ncol=2)
  colnames(errors) <- c("k","LOOCV error")

  for (i in 1:upper.bound)
  {
    myknn.cv <- knn.cv(mydata, mytargets, k = i)
  
    # fill in number of neighbours and LOOCV error
    errors[i, "k"] <- i
  
    tab <- table(Truth=mytargets, Preds=myknn.cv)
    errors[i, "LOOCV error"] <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab)
  }
  errors
}



knn.kfcv <- function(target, dd, num.neighbors, nfolds = 10, seed = 46290) {
  set.seed(seed)
  rownames(dd) <- NULL
  n <- nrow(dd)
  folds <- createFolds(seq_len(n), k = nfolds, list = TRUE)
  t <- NULL
  p <- NULL
  for (fold in folds) {
    adj.data <- dd[-fold,]
    val <- dd[fold,]
    fmla <- as.formula(paste(target,"~.-quality"))
    model <- lda(fmla, data = adj.data)
    adj.lda <- as.matrix(adj.data[,1:11]) %*% as.matrix(model$scaling)
    val.lda <- as.matrix(val[,1:11]) %*% as.matrix(model$scaling)
    predicted.class <- knn(train = adj.lda, test = val.lda, cl = adj.data[,target], k = num.neighbors)
    t <- c(t, val[,target])
    p <- c(p, predicted.class)
  }
  tab <- table(Truth = t, Pred = p)
}
```

```{r}
# knn amb dades originals
set.seed(our_seed)
loocv.error <- loop.k(training.data[,1:11], training.data$type, sqrt(nrow(training.data)))
plot(loocv.error, type="l", xaxt = "n", main = "kNN cv-error (original data)")
axis(1:sqrt(nrow(training.data)))
kmin.1 <- which.min(loocv.error[,2])
min1 <- loocv.error[kmin.1,]

# 
set.seed(our_seed)
loocv.error <- loop.k(scale(training.data[,1:11]), training.data$type, sqrt(nrow(training.data)))
plot(loocv.error, type="l", xaxt = "n", main = "kNN cv-error (scaled data)")
axis(1:sqrt(nrow(training.data)))
kmin.2 <- which.min(loocv.error[,2])
min2 <- loocv.error[kmin.2,]

# 
set.seed(our_seed)
mod.lda <- lda(type~.-quality, data = training.data)
training.lda <- as.matrix(training.data[,1:11]) %*% as.matrix(mod.lda$scaling)
loocv.error.lda <- loop.k(training.lda, training.data$type, sqrt(nrow(training.data)))
plot(loocv.error.lda, type='l', xaxt='n', main = "kNN cv-error (LDA-projected data)")
axis(1:sqrt(nrow(training.data)))
kmin.3 <- which.min(loocv.error.lda[,2])
min3 <- loocv.error[kmin.3,]

(list("minimum with original data" = min1, "minimum with scaled data" = min2, "minimum with LDA-projected data" = min3))
```


```{r}
kfcv.err <- NULL
for (i in 1:25) {
  tab <- knn.kfcv(target="type", dd=training.data, num.neighbors=i)
  kNN.kfcv.err <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab)
  kfcv.err <- c(kfcv.err, kNN.kfcv.err)
}
plot(c(1:25), kfcv.err, xlab = "number of neighbors")

print(kfcv.err)
```


### GLM
```{r}
set.seed(our_seed)
training.data$type <- factor(training.data$type)
k <- 10
glm_model <- train(form = type ~ .-quality,
               data = training.data,
               trControl = trainControl(method = "cv", number = k),
               method = "glm",
               family = "binomial")

# Table of results along with accuracy
(tab <- table(training.data$type, predict(glm_model)))
(glm.kfcv.err <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab))
```


### MLP

```{r}
set.seed(our_seed)
# 10-fold CV with 10 repetitions
trc <- trainControl (method="cv", number=10)
# we will train the model fixing a large number of units and using weight decay
(decays <- 10^seq(-2, 0, by=0.2))

# train the model
mlp.model <- train(form = type ~ .-quality,
               data = training.data,
               trControl = trc,
               method = "nnet",
               maxit = 200,
               tuneGrid = expand.grid(.size=20,.decay=decays),
               trace = FALSE)

# Table of results along with accuracy
(tab <- table(training.data$type, predict(mlp.model)))
(mlp.error <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab))
```

### RBF

```{r}
modRBF<- train(type~., 
    data = training.data, 
    method = "rbfDDA", 
    trControl = trainControl(method = "CV", number = 10))
```


### Random Forests

```{r}
ntrees <- round(2^seq(1,10))
ntrees <- seq(150, 300, 10)


rf.results <- matrix (rep(0,2*length(ntrees)), nrow=length(ntrees))
colnames (rf.results) <- c("ntrees", "OOB")
rf.results[,"ntrees"] <- ntrees
rf.results[,"OOB"] <- 0


ii <- 1
set.seed(our_seed)
training.data$type <- as.factor(training.data$type)
for (nt in ntrees)
{ 
  print(nt)
  
  # build forest
  model.rf <- randomForest(type ~ .-quality, data=training.data, ntree=nt, proximity=FALSE, 
                           sampsize=c(r=round(0.75*895), w=round(0.75*2649)), strata=training.data$type)
  
  # get the OOB and store it appropriately
  rf.results[ii, "OOB"] <- model.rf$err.rate[nt,1]  
  ii <- ii+1
}

rf.results
```

```{r}
rf.kfcv <- function(target, dd, nfolds = 10, seed = 46290, num_trees) {
  set.seed(seed)
  
  rownames(dd) <- NULL
  n <- nrow(dd)
  
  folds <- createFolds(seq_len(n), k = nfolds, list = TRUE)
  
  cv.err <- NULL
  t <- NULL
  p <- NULL
  
  for (fold in folds) {
    adj.data <- dd[-fold,]
    val <- dd[fold,]
    fmla <- as.formula(paste(target,"~.-quality"))
    
    model.rf1 <- randomForest (fmla, data=adj.data, ntree=num_trees, proximity=FALSE)
    pred.rf1 <- predict (model.rf1, val, type="class")
    
    t <- c(t, val[,target])
    p <- c(p, pred.rf1)
  }
  
  tab <- table(Truth = t, Pred = p)
}
```


```{r}
(tab <- rf.kfcv("type", training.data, num_trees=290))
(rf.kfcv.err <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab))
```



## QUALITY PREDICTION

### Classification

```{r}
set.seed(our_seed)
rda.model.10foldcv <- rda(quality~., data = training.data, crossval = TRUE, fold = 10)
rda.model.10foldcv$regularization
```


```{r}
lda.kfcv <- function(target, dd, nfolds = 10, seed = 46290) {
  set.seed(seed)
  rownames(dd) <- NULL
  n <- nrow(dd)
  folds <- createFolds(seq_len(n), k = nfolds, list = TRUE)
  cv.err <- NULL
  t <- NULL
  p <- NULL
  for (fold in folds) {
    adj.data <- dd[-fold,]
    val <- dd[fold,]
    fmla <- as.formula(paste(target,"~."))
    lda.pred <- lda(fmla, data = adj.data) %>% predict(newdata = val)
    predicted.class <- lda.pred$class
    t <- c(t, val[,target])
    p <- c(p, predicted.class)
  }
  tab <- table(Truth = t, Pred = p)
}

(tab <- lda.kfcv(target = "quality", training.data, nfolds = 10))
(LDA.kfcv.err <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab))
```



### Regression
```{r}
set.seed(our_seed)

k <- 10
training.data$type <- as.factor(training.data$type)
quality.glm.model <- train(form = quality ~ .,
               data = training.data,
               trControl = trainControl(method = "cv", number = k),
               method = "lm")

# Table of results along with accuracy
preds <- round(predict(quality.glm.model))
(linearRegression.kfcv.err <- mean((preds - training.data$quality)^2))
```

#### Ridge

```{r}
lambdas <- seq(6.5,7,0.01)

model.ridge <- lm.ridge(quality~., data = training.data, lambda = lambdas)
plot(lambdas, model.ridge$GCV, main="GCV of Ridge Regression", type="l", 
     xlab=expression(lambda), ylab="GCV")

# The optimal lambda is given by
(lambda.ridge <- lambdas[which.min(model.ridge$GCV)])
abline (v=lambda.ridge,lty=2)
```

```{r}
training.data$type <- as.factor(training.data$type)
ridge.mod <- lm.ridge(quality~., data = training.data, lambda = lambda.ridge)
(beta.ridge <- coef(ridge.mod))
temp.ridge.pred <- beta.ridge[1] + as.matrix(training.data[,1:11]) %*% beta.ridge[2:12]
ridge.pred <- round(temp.ridge.pred)



ridge.kfcv <- function(target, dd, nfolds = 10, seed = 46290) {
  set.seed(seed)
  rownames(dd) <- NULL
  n <- nrow(dd)
  folds <- createFolds(seq_len(n), k = nfolds, list = TRUE)
  cv.err <- NULL
  t <- NULL
  p <- NULL
  for (fold in folds) {
    adj.data <- dd[-fold,]
    val <- dd[fold,]
    fmla <- as.formula(paste(target,"~."))
    ridge.mod <- lm.ridge(fmla, data = adj.data, lambda = lambda.ridge)
    beta.ridge <- coef(ridge.mod)
    temp.ridge.pred <- beta.ridge[1] + as.matrix(val) %*% beta.ridge[2:12]
    ridge.pred <- round(temp.ridge.pred)
    t <- c(t, val[,target])
    p <- c(p, ridge.pred)
  }
  mse <- mean((t-p)^2)
}

(ridge.kfcv.err <- ridge.kfcv("quality", training.data))

```


#### LASSO

```{r}
library(Matrix)
library(glmnet)

t <- as.matrix(training.data[,1:11], ncol = 11)

training.w <- t[training.data$type == "w",]
training.r <- t[training.data$type == "r",]

model.lasso.red <- cv.glmnet(as.matrix(training.w), as.numeric(training.data[training.data$type == "w",]$quality), nfolds = 10)
model.lasso.white <- cv.glmnet(as.matrix(training.r), as.numeric(training.data[training.data$type == "r",]$quality), nfolds = 10)

(lambda.lasso.w <- model.lasso.white$lambda.min)
(lambda.lasso.r <- model.lasso.red$lambda.min)

plot(model.lasso.red)
red.kfcv.err <- model.lasso.red$cvm[model.lasso.red$lambda == lambda.lasso.r]
white.kfcv.err <- model.lasso.red$cvm[model.lasso.red$lambda == lambda.lasso.r]

n.red <- nrow(training.r)
n.white <- nrow(training.w)

(LASSO.kfcv.err <- (n.red * red.kfcv.err + n.white * white.kfcv.err)/( n.red + n.white ) ) # error total, diria que la fÃ³rmula Ã©s correcta.
```

