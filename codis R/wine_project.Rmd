---
title: "wine project"
author: "Ã€lex, Luis, Aleix"
date: "25/4/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(class)
library(dplyr)
library(klaR)
library(magrittr)
library(caret)
```



```{r}
our_seed <- 46290
red_w <- read.table("winequality-red.csv", sep=';', header = TRUE)
white_w <- read.table("winequality-white.csv", sep=';', header = TRUE)
```


```{r}
head(red_w)
dim(red_w)
head(white_w)
dim(white_w)
```

```{r}
red_w <- distinct(red_w)
white_w <- distinct(white_w)

wine <- rbind(red_w, white_w)

red_w <- red_w[c(-723,-964),]
white_w <- white_w[c(-2175, -2765),]
wine <- wine[c(-723,-964),]
```

```{r}
apply(red_w, 2, hist)
```

```{r}
wine <- rbind(red_w, white_w)
wine$type <- c(rep('r', nrow(red_w)), rep('w', nrow(white_w)))
wine.quality <- wine$quality
wine.data <- subset(wine, select = 1:11)
```


Separate data into test and training
```{r}
# Randomly separate into learning data and test data
set.seed(our_seed)
sample_size <- floor(2/3 * nrow(wine))
training_indices <- sample(seq_len(nrow(wine)), size = sample_size)
training.data  <- wine[training_indices, ]
testing.data <- wine[-training_indices, ]
```

## TYPE PREDICTION
```{r}
lda.kfcv <- function(target, dd, nfolds = 10, seed = 46290) {
  set.seed(seed)
  rownames(dd) <- NULL
  n <- nrow(dd)
  folds <- createFolds(seq_len(n), k = nfolds, list = TRUE)
  cv.err <- NULL
  t <- NULL
  p <- NULL
  for (fold in folds) {
    adj.data <- dd[-fold,]
    val <- dd[fold,]
    fmla <- as.formula(paste(target,"~.-quality"))
    lda.pred <- lda(fmla, data = adj.data) %>% predict(newdata = val)
    predicted.class <- lda.pred$class
    t <- c(t, val[,target])
    p <- c(p, predicted.class)
  }
  tab <- table(Truth = t, Pred = p)
}

(tab <- lda.kfcv(target = "type", training.data, nfolds = 10))
(LDA.kfcv.err <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab))

lda.model <- lda(type ~ .-quality, data = training.data)
loadings.lda <- as.matrix(training.data[,1:11]) %*% as.matrix(lda.model$scaling)
#plot(loadings.lda, col=c(training.data$type), ylab = "Projected coordinates", main = "LDA projection")
```


```{r}
loop.k <- function(mydata, mytargets, upper.bound)
{
  errors <- matrix(nrow=upper.bound, ncol=2)
  colnames(errors) <- c("k","LOOCV error")

  for (i in 1:upper.bound)
  {
    myknn.cv <- knn.cv(mydata, mytargets, k = i)
  
    # fill in number of neighbours and LOOCV error
    errors[i, "k"] <- i
  
    tab <- table(Truth=mytargets, Preds=myknn.cv)
    errors[k, "LOOCV error"] <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab)
  }
  errors
}



knn.kfcv <- function(target, dd, num.neighbors, nfolds = 10, seed = 46290) {
  set.seed(seed)
  rownames(dd) <- NULL
  n <- nrow(dd)
  folds <- createFolds(seq_len(n), k = nfolds, list = TRUE)
  t <- NULL
  p <- NULL
  for (fold in folds) {
    adj.data <- dd[-fold,]
    val <- dd[fold,]
    fmla <- as.formula(paste(target,"~.-quality"))
    model <- lda(fmla, data = adj.data)
    adj.lda <- as.matrix(adj.data[,1:11]) %*% as.matrix(model$scaling)
    val.lda <- as.matrix(val[,1:11]) %*% as.matrix(model$scaling)
    predicted.class <- knn(train = adj.lda, test = val.lda, cl = adj.data[,target], k = num.neighbors)
    t <- c(t, val[,target])
    p <- c(p, predicted.class)
  }
  tab <- table(Truth = t, Pred = p)
}
```

```{r}
kfcv.err <- NULL
for (i in 4:25) {
  tab <- knn.kfcv(target="type", dd=training.data, num.neighbors=i)
  kNN.kfcv.err <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab)
  kfcv.err <- c(kfcv.err, kNN.kfcv.err)
}
plot(c(4:25), kfcv.err)

print(kfcv.err)

#set.seed(our_seed)
#loocv.error <- loop.k(training.data, training.data$type, sqrt(nrow(training.data)))
#plot(loocv.error, type="l", xaxt = "n")
#axis(1, neighbours)
#kmin <- which.min(loocv.error[,2])
#loocv.error[kmin,]
```


```{r}
#set.seed(our_seed)
#loocv.error <- loop.k(scale(wine.data), wine$type, neighbours)
#plot(loocv.error, type="l", xaxt = "n")
#axis(1, neighbours)
#kmin <- which.min(loocv.error[,2])
#loocv.error[kmin,]
```

```{r}
set.seed(our_seed)
loocv.error.lda <- loop.k(loadings.lda, wine$type, neighbours)
plot(loocv.error.lda, type='l')#, xaxt='n')
kmin <- which.min(loocv.error.lda[,2])
loocv.error.lda[kmin,]
```


```{r}
set.seed(our_seed)
lda.model.loocv <- lda(x=wine.data, grouping=wine$type, CV=TRUE)
(ct <- table(Truth=wine$type, Pred=lda.model.loocv$class))
1 - sum(ct[row(ct)==col(ct)])/sum(ct)
```

```{r}
set.seed(our_seed)
qda.model.loocv <- qda(x=wine.data, grouping=wine$type, CV=TRUE)
(ct <- table(Truth=wine$type, Pred=qda.model.loocv$class))
1 - sum(ct[row(ct)==col(ct)])/sum(ct)
```


```{r}
set.seed(our_seed)
rda.model.loocv <- rda(x=wine.data, grouping=wine$type, fold=nrow(wine.data))
rda.model.loocv$regularization
```
We can see how lambda is 1 and gamma is very close to zero, so we should use LDA.


#### GLM
```{r}
set.seed(our_seed)
training.data$type <- factor(training.data$type)
# We use LOOCV, so we will use nrow(wine.data) folds
k <- nrow(wine.data)
glm_model <- train(form = type ~ .-quality,
               data = training.data,
               trControl = trainControl(method = "cv", number = k),
               method = "glm",
               family = "binomial")

# Table of results along with accuracy
table(training.data$type, predict(glm_model))
glm_model

# Real accuracy as calculated over test data
glm_predicted <- glm_model %>% predict(testing.data, type = "raw")
(glm_table <- table(testing.data$type, glm_predicted))
(glm_error <- 1 - sum(glm_table[row(glm_table)==col(glm_table)])/sum(glm_table))
```

## QUALITY PREDICTION

### Regression
```{r}
set.seed(our_seed)
# We use LOOCV, so we will use nrow(wine.data) folds
k <- nrow(wine.data)
training.data$type <- as.factor(training.data$type)
testing.data$type <- as.factor(testing.data$type)
quality.glm.model <- train(form = quality ~ .,
               data = training.data,
               trControl = trainControl(method = "cv", number = k),
               method = "lm")

# Table of results along with accuracy
table(Truth=training.data$quality, Pred=round(predict(quality.glm.model)))
quality.glm.model

# Real accuracy as calculated over test data
glm.predicted <- quality.glm.model %>% predict(testing.data, type = "raw")
glm.predicted <- round(glm.predicted)
(glm.table <- table(Truth=testing.data$quality, Pred=glm.predicted))
(glm.error <- 1 - sum(glm.table[row(glm.table)==col(glm.table)])/sum(glm.table))
```
#### Ridge
#### LASSO

### Classification

